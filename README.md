# Course-Evaluation-Prediction-with-Streamlit-

Given a dataset of course evaluations written by students across a myriad of courses at UM, I'm modeling the features (‘Division’, ‘Type’, and ‘Comment.Text.Processed’) to predict a real-value target variable (‘score’). The model evaluation metric will be mean squared error. I'm conducting data cleaning, exploratory data analysis to understand the data available, and feature engineering and preprocessing to get the training and validation sets prepared to model. For the Unsupervised Learning portion, I will explore two techniques to topic model evaluations (Latent Dirichlet Allocation and Non-negative Matrix Factorization) and assess ways to empower the supervised learning portion. This file is specifically just using streamlit to display the modeling techniques in visual ways on a webpage. 

The final report also contains more techniques to assess performance and explain model performance in a more robust way. The report contains hyperparameter sensitivity analyses, feature importance through ablation testing, residual normality tests, and failure analysis for the supervised modeling. The unsupervised modeling is assessed for interpretation and quality through coherence score and jaccard similarity. The discussion below is the result after the extensive analysis.


-- Discussion --

As I conducted Supervised Learning, I learned most about: the value of simple solutions, the significant effort to design and implement meaningful features, the computational challenges to be able to iterate on ideas quickly, and how to consistently balance tradeoffs. Even with trying out a variety of different models, the Linear Regression (which was the first one tried) ended up with the best results at the end. As model complexity grew, I spent more time in tuning the models to find that they still would not beat the Linear Regression one. Occam’s Razor really stuck out here of deferring to simpler solutions/explanations unless more complexity is a necessity. Secondly, I also learned that the features had a far more meaningful impact in the results and when designing/creating features that could differentiate low scores more specifically, I learned how much effort that would require. It was a venture that would require more time with the data and understanding where the data comes from to be able to construct non-generic features. This reminded me of the significance of domain value knowledge in data science projects. Additionally, as I was running a variety of models through via grid search and cross validation, it became clear how long some took and how limiting that was. I had to rethink the hyperparameter tuning to not just include as wide of a range for any features but be more specific and intentional in my choices. This also proved to be valuable when attempting to explain the models at the end. Finally, I learned that assessing tradeoffs for modeling is not only relevant when assessing model performance with metrics, but it is something that should be done throughout the whole process. Knowing when exhaustively tuning a model or preprocessing technique or creating more features is an art as much as a science. I had to intentionally think of when to stop tuning a model for marginal performance improvements and only focus on what mattered (feature vectorizer tuning or feature engineering). I was primarily surprised at the effectiveness of Linear Regression models and the impact that TfidfVectorizer tuning had on model performance. In hindsight, this does seem to make sense as there are a limited number of features, but it was interesting to work through a problem that required you to actually think about the data and not just throw all the ‘recommended strategies’ at it. For ex. When I was applying idf weighting, the features at the end seemed a little nonsensical. After only leveraging term frequencies, model performance increased and was more explainable.
With more time, I would extend the solution in these ways: a logistic regression model to first differentiate the data into binary ‘positive’ and ‘negative’ sentiment classifications before the regression modeling, utilizing more finely tuned topic modeling (such as turbo topics) to create more meaningful features that differentiate the data before the regression modeling, and tuning XGBoost to optimize performance. I believe logistic regression would be a suitable model to find high and low scores with a balanced set of classes (could use SMOTE or random sampling to keep high and low score evaluation classes balanced) and the topic modeling could illuminate latent structural patterns that emerge in lower scores which are unable to discern easily. Additionally, I believe that there are more performance improvements that could be achieved through tuning XGBoost considering how well it performed out of the box.
Implementing supervised modeling as a solution could raise some ethical concerns. The first is that students from varied backgrounds convey language in different ways. What is sarcastic in one culture may be less hurtful elsewhere. Trying to only use language and words expressed makes it hard to differentiate whether this may be a cause. Additionally, the question is still outstanding of who this solution is really to serve and how? We should check whether there is a pattern in the learned models to detect for age, gender, and race. There could be the case that some groups have a specific issue with the class/professor but are drowned out due to the low frequency of representation. Finally, there is the issue of privacy and informed consent. As students are entering these values, they are under the impression that it is completely anonymous. It could be likely that a student may not have entered the same response in if they believed it would somehow get back to the professor or faculty. Some ways to address these issues would be to run homogeneity tests to check if there is balance amongst race, age, and gender variables. Showing significant patterns in these features could indicate that our model is learning some implicit patterns we do not intend it to. Additionally, including linguistic professionals and social scientists in the conversation would be helpful to identify if there are patterns in the language that may be improperly interpreted across different cultures. Finally, I would add an ‘Opt In’ option on the course evaluation surveys that clearly and concisely inform the student how the data would be used and provide them the option to allow their entered data to be used. Students may also choose after some time to change their minds, so each student who opted in should receive an email with clear instructions on how to opt out if they wish to which would purge their stored data.
As I conducted Unsupervised Learning, I learned most about: the difficulty in topic modeling with a large frequency of similar words, the impact in model learning based on using term frequency or tf-idf weighting, and the tradeoffs with using LDA and NMF. Latent Dirichlet Allocation is a probabilistic (generative) model is finding the distribution of words that best explain the original corpus, but if the original corpus has quite a skewed distribution then you will naturally result in what we saw – a lot of topic models showing similar words with the highest term frequency. Non-Negative Matrix Factorization attempts to decompose a matrix into a small number of matrix factors and the columns of one of these matrix factors can be considered topics. Since this takes tf-idf weighting as an input, I found that some of the values in each topic were more varied than just top term frequencies but still had too similar words amongst each topic model. It did surprise me that the models did not perform as well, and the amount of effort needed to carefully tune them properly.
With more time, I would extend the solution in these ways: a robust study on hyperparameter sensitivity analysis to see which hyperparameters affect each model strongly, utilizing turbo topics to get more intelligible topics, and diving deeper into the demographics to find more interesting patterns amongst different professors.
Implementing unsupervised learning as a solution could also raise significant ethical concerns. Unsupervised modeling contains the same privacy & informed consent issue, so I believe a similar strategy to address it would be appropriate – adding an ‘Opt In’ option where students are clearly aware of what is being used, how, why, and how to change their mind if needed. Unsupervised Learning would require careful monitoring especially on the language ethical concerns as it is looking to find latent structure that cannot be easily visible. This would mean ensuring that underlying data distributions match the training data and are not misrepresenting the information provided is a critical factor. I would address this by adding a human in the loop to routinely monitor the topic models and a random subsample of the training data to see if the topic models are reasonably accurate along with the metrics such as Coherence Score and Jaccard Similarity.
